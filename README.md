# nlp-disaster-tweets-bertforsequenceclassif
Python notebook using data from Real or Not? NLP with Disaster Tweets (Kaggle)
==============================================
[My Linkedin](https://www.linkedin.com/in/sabirmakhlouf) <br />
[My Facebook](https://www.facebook.com/MakhloufSabir) <br />
[My twitter](https://twitter.com/Sabir_Makhlouf) <br />

I used transfer learning (BERT model).

[Kaggle Notebook](https://www.kaggle.com/makhloufsabir/nlp-disaster-tweets-bertforsequenceclassif) <br />

1- Read Data - Data Preprocessing
2- Data Transforming (TOKEN, PADDING, MASKING)
3- Create torch tensors
4- Modelling (use TL: BertForSequenceClassification)

Validation Accuracy: 0.8129340277777778
Public Score: 0.82071 (you can increase it by advanced data preprocessing skills)


**![](https://i.imgur.com/ipk2hRD.png)**

**![](https://i.imgur.com/w8zzXRn.png)**
**![](https://i.imgur.com/Neoj4wY.png)**





